import asyncio
import pandas as pd
from playwright.async_api import async_playwright

# æ±‚äººæƒ…å ±ã‚µã‚¤ãƒˆã®ãƒ™ãƒ¼ã‚¹URLã¨æ¤œç´¢ãƒ‘ã‚¹
BASE_URL = "https://xn--pckua2a7gp15o89zb.com"
SEARCH_PATH = "/%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9E%E3%83%BC%E3%81%AE%E4%BB%95%E4%BA%8B-%E6%9D%B1%E4%BA%AC%E9%83%BD?e=1"

# å…¨æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
all_job_data = []

# å„æ±‚äººã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
async def extract_job_details(page, url):
    try:
        # æ±‚äººè©³ç´°ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        await page.goto(url, timeout=60000)

        # ã‚»ãƒ¬ã‚¯ã‚¿ã«å¯¾å¿œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹å†…éƒ¨é–¢æ•°
        async def get_text(selector):
            locator = page.locator(selector)
            try:
                if await locator.count() > 0:
                    content = await locator.text_content()
                    return content.strip() if content else ""
                else:
                    return ""
            except Exception:
                return ""

        # æ±‚äººæƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§è¿”ã™
        return {
            "æ±‚äººã‚¿ã‚¤ãƒˆãƒ«": await get_text("p.p-detail_head_title"),
            "ä¼æ¥­å": await get_text("p.p-detail_head_company"),
            "æ‰€åœ¨åœ°": await get_text("li.p-detail_summary.p-detail_summary-area.c-icon--U2"),
            "çµ¦ä¸": await get_text("li.p-detail_summary.p-detail_summary-pay.c-icon--V2"),
            "é›‡ç”¨å½¢æ…‹": await get_text("li.p-detail_tag.p-detail_tag-employType"),
        }

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}ï¼ˆURL: {url}ï¼‰")
        return None

# æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã€å„æ±‚äººã®è©³ç´°URLã«ã‚¢ã‚¯ã‚»ã‚¹
async def scrape_page(page, url):
    # æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
    await page.goto(url)
    # æ±‚äººãƒªã‚¹ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
    await page.wait_for_selector('[data-target-focus]')

    # æ±‚äººIDã‚’å–å¾—ï¼ˆdata-target-focus å±æ€§ã‹ã‚‰æŠ½å‡ºï¼‰
    job_ids = await page.locator('[data-target-focus]').evaluate_all(
        """(elements) => {
            return elements
                .map(el => el.getAttribute('data-target-focus'))
                .filter(id => id);
        }"""
    )

    # å„æ±‚äººã®è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
    detail_urls = [f"{BASE_URL}/jb/{job_id}" for job_id in job_ids]
    print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL: {url} - æ±‚äººæ•°: {len(detail_urls)}")

    # å„è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã€ãƒªã‚¹ãƒˆã«è¿½åŠ 
    for detail_url in detail_urls:
        job_info = await extract_job_details(page, detail_url)
        if job_info:
            all_job_data.append(job_info)

# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆPlaywrightã®èµ·å‹•ã¨å®Ÿè¡Œï¼‰
async def main():
    async with async_playwright() as p:
        # Chromiumãƒ–ãƒ©ã‚¦ã‚¶ã‚’éè¡¨ç¤ºï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ï¼‰ã§èµ·å‹•
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # æ¤œç´¢çµæœã®1ãƒšãƒ¼ã‚¸ç›®ã¨2ãƒšãƒ¼ã‚¸ç›®ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        for page_num in range(1, 3):
            full_url = f"{BASE_URL}{SEARCH_PATH}&pg={page_num}"
            await scrape_page(page, full_url)

        # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
        await browser.close()

    # å…¨æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    df = pd.DataFrame(all_job_data)
    df.to_excel("æ±‚äººæƒ…å ±ä¸€è¦§.xlsx", index=False)
    print("ğŸ“ Excelã«ä¿å­˜å®Œäº†: æ±‚äººæƒ…å ±ä¸€è¦§.xlsx")

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
if __name__ == "__main__":
    asyncio.run(main())

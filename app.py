import asyncio
import pandas as pd
from playwright.async_api import async_playwright

BASE_URL = "https://xn--pckua2a7gp15o89zb.com"
SEARCH_PATH = "/%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9E%E3%83%BC%E3%81%AE%E4%BB%95%E4%BA%8B-%E6%9D%B1%E4%BA%AC%E9%83%BD?e=1"

all_job_data = []


async def extract_job_details(page, url):
    try:
        await page.goto(url, timeout=60000)

        async def get_text(selector):
            locator = page.locator(selector)
            try:
                if await locator.count() > 0:
                    content = await locator.text_content()
                    return content.strip() if content else ""
                else:
                    return ""
            except Exception:
                return ""

        return {
            "æ±‚äººã‚¿ã‚¤ãƒˆãƒ«": await get_text("p.p-detail_head_title"),
            "ä¼æ¥­å": await get_text("p.p-detail_head_company"),
            "æ‰€åœ¨åœ°": await get_text("li.p-detail_summary.p-detail_summary-area.c-icon--U2"),
            "çµ¦ä¸": await get_text("li.p-detail_summary.p-detail_summary-pay.c-icon--V2"),
            "é›‡ç”¨å½¢æ…‹": await get_text("li.p-detail_tag.p-detail_tag-employType"),
        }

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}ï¼ˆURL: {url}ï¼‰")
        return None


async def scrape_page(page, url):
    await page.goto(url)
    await page.wait_for_selector('[data-target-focus]')

    job_ids = await page.locator('[data-target-focus]').evaluate_all(
        """(elements) => {
            return elements
                .map(el => el.getAttribute('data-target-focus'))
                .filter(id => id);
        }"""
    )

    detail_urls = [f"{BASE_URL}/jb/{job_id}" for job_id in job_ids]
    print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL: {url} - æ±‚äººæ•°: {len(detail_urls)}")

    for detail_url in detail_urls:
        job_info = await extract_job_details(page, detail_url)
        if job_info:
            all_job_data.append(job_info)


async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        for page_num in range(1, 3):  # 1ãƒšãƒ¼ã‚¸ç›®ã¨2ãƒšãƒ¼ã‚¸ç›®
            full_url = f"{BASE_URL}{SEARCH_PATH}&pg={page_num}"
            await scrape_page(page, full_url)

        await browser.close()

    df = pd.DataFrame(all_job_data)
    df.to_excel("æ±‚äººæƒ…å ±ä¸€è¦§.xlsx", index=False)
    print("ğŸ“ Excelã«ä¿å­˜å®Œäº†: æ±‚äººæƒ…å ±ä¸€è¦§.xlsx")

if __name__ == "__main__":
    asyncio.run(main())
